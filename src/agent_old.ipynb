{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "115677ec-cb07-415e-8539-5173ba3deb56",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-28 22:05:08.854885: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from collections import deque\n",
    "import random\n",
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "742201ff-2543-4202-8ac9-1342fa45d92e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define trading environment\n",
    "class TradingEnv:\n",
    "    def __init__(self, window_size=10, steps=100):\n",
    "        self.window_size = window_size\n",
    "        self.startBalance = 1000.0\n",
    "        self.steps = steps\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.data = json.loads(requests.get('http://192.168.50.100:6000/binance/btcusdt').text)\n",
    "        self.current_step = self.window_size\n",
    "        self.bought_price = 0\n",
    "        self.sold_price = 0\n",
    "        self.balance = 1000.0\n",
    "        self.bought = False\n",
    "        return self.data\n",
    "\n",
    "    def _take_action(self, action):\n",
    "        current_price = self.data['bookTicker']['a']\n",
    "\n",
    "        reward = 0\n",
    "\n",
    "        if action == 0 and not self.bought: # Buy\n",
    "            \n",
    "            if (self.sold_price > current_price):\n",
    "                reward += self.sold_price / current_price\n",
    "            else:\n",
    "                reward -= self.sold_price / current_price\n",
    "\n",
    "            self.bought_price = current_price\n",
    "            self.bought = True\n",
    "\n",
    "        elif action == 1 and self.bought: # Sell\n",
    "\n",
    "            self.balance = self.balance * current_price / self.bought_price\n",
    "            reward = 0\n",
    "\n",
    "            if (self.bought_price < current_price):\n",
    "                reward += self.bought_price / current_price\n",
    "            else:\n",
    "                reward -= self.bought_price / current_price\n",
    "                \n",
    "            self.bought = False\n",
    "            self.sold_price = current_price\n",
    "\n",
    "        elif action == 2: # Wait\n",
    "            pass\n",
    "\n",
    "        return reward\n",
    "\n",
    "    def step(self, action):\n",
    "        reward = self._take_action(action)\n",
    "        self.current_step += 1\n",
    "        done = self.current_step == self.steps\n",
    "        obs = self.data\n",
    "        return obs, reward, done, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "322fbf5b-391b-410c-994f-1bfd12f5eac9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define the RL model\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size, learning_rate=0.000001, gamma=0.95, epsilon=1.0, epsilon_min=0.01,\n",
    "                 epsilon_decay=0.999, target_update_freq=10, batch_size=32000):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=32000)\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.learning_rate = learning_rate\n",
    "        self.target_update_freq = target_update_freq\n",
    "        self.batch_size = batch_size\n",
    "        self.timestep = 0\n",
    "        self.model = self._build_model()\n",
    "        self.target_model = self._build_model()\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def _build_model(self):\n",
    "        input_layer = Input(shape=self.state_size)\n",
    "        x = Dense(32, activation=\"relu\")(input_layer)\n",
    "        x = Dense(64, activation=\"relu\")(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "        x = Dense(32, activation=\"relu\")(x)\n",
    "        output_layer = Dense(self.action_size, activation=\"linear\")(x)\n",
    "        model = Model(inputs=input_layer, outputs=output_layer)\n",
    "        model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(learning_rate=self.learning_rate), metrics=['accuracy'])\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "334a7a6f-6c95-4341-ab55-70de5bf7783d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize trading environment\n",
    "env = TradingEnv()\n",
    "\n",
    "# Set hyperparameters\n",
    "\n",
    "state_size = env.window_size * 4\n",
    "action_size = 3\n",
    "num_episodes = 100\n",
    "\n",
    "# Initialize agent\n",
    "agent = DQNAgent(state_size, action_size)\n",
    "\n",
    "# Set constants\n",
    "total_reward = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "138c209c-c10f-4e11-a5b0-07d05a2aad1b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type dict).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m     action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(action_size)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 16\u001b[0m     q_values \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_array\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(q_values[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     19\u001b[0m next_state, reward, done, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py:103\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    101\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[1;32m    102\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type dict)."
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "for episode in range(num_episodes):\n",
    "    \n",
    "    state = env.reset()\n",
    "    \n",
    "    state_array = np.array(list(state.values()))\n",
    "    state_size = len(state_array)\n",
    "    next_state_array = np.reshape(state_array, [1, state_size])\n",
    "    \n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        if np.random.rand() <= agent.epsilon:\n",
    "            action = np.random.randint(action_size)\n",
    "        else:\n",
    "            q_values = agent.model.predict(state_array, verbose=0)\n",
    "            action = np.argmax(q_values[0])\n",
    "\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        next_state_array = np.array(list(next_state.values()))\n",
    "        state_size = len(next_state_array)\n",
    "        next_state_array = np.reshape(next_state_array, [1, state_size])\n",
    "        total_reward += reward\n",
    "\n",
    "        # Store experience in replay memory\n",
    "        agent.memory.append((state, action, reward, next_state_array, done))\n",
    "\n",
    "        # Update the state\n",
    "        state = next_state_array\n",
    "\n",
    "        # Perform experience replay if memory is full\n",
    "        if len(agent.memory) >= agent.batch_size:\n",
    "            batch = random.sample(agent.memory, agent.batch_size)\n",
    "            for state, action, reward, next_state_array, done in batch:\n",
    "                if done:\n",
    "                    target = reward\n",
    "                else:\n",
    "                    target = reward + agent.gamma * np.amax(agent.model.predict(next_state_array, verbose=0)[0])\n",
    "\n",
    "                target_f = agent.model.predict(state, verbose=0)\n",
    "                target_f[0][action] = target\n",
    "                agent.model.fit(state, target_f, epochs=1, verbose=1)\n",
    "\n",
    "        # Decay epsilon\n",
    "        if agent.epsilon > agent.epsilon_min:\n",
    "            agent.epsilon *= agent.epsilon_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88dbe00-7f54-4dc4-80fb-e7159c76315e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Total Reward = {}\".format(total_reward))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
