{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "115677ec-cb07-415e-8539-5173ba3deb56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from collections import deque\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "742201ff-2543-4202-8ac9-1342fa45d92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define trading environment\n",
    "class TradingEnv:\n",
    "    def __init__(self, window_size=10):\n",
    "        self.data = np.random.randint(1, 100, size=(100, 2))\n",
    "        self.window_size = window_size\n",
    "        self.startBalance = 1000.0\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = self.window_size\n",
    "        self.bought_price = 0\n",
    "        self.sold_price = 0\n",
    "        self.balance = 1000.0\n",
    "        self.bought = False\n",
    "        return self._next_observation()\n",
    "\n",
    "    def _next_observation(self):\n",
    "        return self.data[self.current_step - self.window_size:self.current_step]\n",
    "\n",
    "    def _take_action(self, action):\n",
    "        current_price = self.data[self.current_step][0]\n",
    "\n",
    "        reward = 0\n",
    "\n",
    "        if action == 0 and not self.bought: # Buy\n",
    "            \n",
    "            if (self.sold_price > current_price):\n",
    "                reward += self.sold_price / current_price\n",
    "            else:\n",
    "                reward -= self.sold_price / current_price\n",
    "\n",
    "            self.bought_price = current_price\n",
    "            self.bought = True\n",
    "\n",
    "        elif action == 1 and self.bought: # Sell\n",
    "\n",
    "            self.balance = self.balance * current_price / self.bought_price\n",
    "            reward = 0\n",
    "\n",
    "            if (self.bought_price < current_price):\n",
    "                reward += self.bought_price / current_price\n",
    "            else:\n",
    "                reward -= self.bought_price / current_price\n",
    "                \n",
    "            self.bought = False\n",
    "            self.sold_price = current_price\n",
    "\n",
    "        elif action == 2: # Wait\n",
    "            pass\n",
    "\n",
    "        return reward\n",
    "\n",
    "    def step(self, action):\n",
    "        reward = self._take_action(action)\n",
    "        self.current_step += 1\n",
    "        done = self.current_step == len(self.data)\n",
    "        obs = self._next_observation()\n",
    "        return obs, reward, done, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "322fbf5b-391b-410c-994f-1bfd12f5eac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the RL model\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size, learning_rate=0.000001, gamma=0.95, epsilon=1.0, epsilon_min=0.01,\n",
    "                 epsilon_decay=0.999, target_update_freq=10, batch_size=32000):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=32000)\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.learning_rate = learning_rate\n",
    "        self.target_update_freq = target_update_freq\n",
    "        self.batch_size = batch_size\n",
    "        self.timestep = 0\n",
    "        self.model = self._build_model()\n",
    "        self.target_model = self._build_model()\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def _build_model(self):\n",
    "        input_layer = Input(shape=self.state_size)\n",
    "        x = Dense(32, activation=\"relu\")(input_layer)\n",
    "        x = Dense(64, activation=\"relu\")(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "        x = Dense(32, activation=\"relu\")(x)\n",
    "        output_layer = Dense(self.action_size, activation=\"linear\")(x)\n",
    "        model = Model(inputs=input_layer, outputs=output_layer)\n",
    "        model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(learning_rate=self.learning_rate), metrics=['accuracy'])\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "334a7a6f-6c95-4341-ab55-70de5bf7783d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize trading environment\n",
    "env = TradingEnv()\n",
    "\n",
    "# Set hyperparameters\n",
    "state_size = env.window_size * env.data.shape[1]\n",
    "action_size = 3\n",
    "num_episodes = 100\n",
    "\n",
    "# Initialize agent\n",
    "agent = DQNAgent(state_size, action_size)\n",
    "\n",
    "# Set constants\n",
    "total_reward = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "138c209c-c10f-4e11-a5b0-07d05a2aad1b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|████████████████████████████████████████| 100/100 [100%] in 4:27.5 (0.37/s) \n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "         if np.random.rand() <= agent.epsilon:\n",
    "            action = np.random.randint(action_size)\n",
    "        else:\n",
    "            q_values = agent.model.predict(state, verbose=0)\n",
    "            action = np.argmax(q_values[0])\n",
    "\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        total_reward += reward\n",
    "\n",
    "        # Store experience in replay memory\n",
    "         agent.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "        # Update the state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform experience replay if memory is full\n",
    "        if len(agent.memory) >= agent.batch_size:\n",
    "            batch = random.sample(agent.memory, agent.batch_size)\n",
    "            for state, action, reward, next_state, done in batch:\n",
    "                if done:\n",
    "                    target = reward\n",
    "                else:\n",
    "                    target = reward + agent.gamma * np.amax(agent.model.predict(next_state, verbose=0)[0])\n",
    "\n",
    "                target_f = agent.model.predict(state, verbose=0)\n",
    "                target_f[0][action] = target\n",
    "                agent.model.fit(state, target_f, epochs=1, verbose=1)\n",
    "\n",
    "        # Decay epsilon\n",
    "        if agent.epsilon > agent.epsilon_min:\n",
    "            agent.epsilon *= agent.epsilon_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a88dbe00-7f54-4dc4-80fb-e7159c76315e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Reward = 288.5261917493278\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Reward = {}\".format(total_reward))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
